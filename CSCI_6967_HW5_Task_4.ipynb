{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTDTXxEpkkn5",
        "outputId": "60cc96bf-841f-4b48-bf69-3bde36afc121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import tifffile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WplKYHzm9cWX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cvc_clinicdb_splits(\n",
        "    original_dir,     # Path to the \"Original/\" folder\n",
        "    mask_dir,         # Path to the \"Ground Truth/\" folder\n",
        "    output_dir,       # Create \"train/\", \"val/\", \"test/\" subfolders\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Splits the CVC-ClinicDB dataset into train/val/test.\n",
        "    Each image in 'original_dir' must have a matching mask in 'mask_dir'.\n",
        "    We copy them into separate subfolders in 'output_dir'.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Gather all image filenames\n",
        "    # Adjust extensions if your images are .jpg, .png, etc.\n",
        "    valid_ext = ('.jpg', '.jpeg', '.png', '.tif')\n",
        "    all_images = [\n",
        "        f for f in sorted(os.listdir(original_dir))\n",
        "        if f.lower().endswith(valid_ext)\n",
        "    ]\n",
        "    if not all_images:\n",
        "        raise ValueError(f\"No image files found in {original_dir} with extensions {valid_ext}\")\n",
        "\n",
        "    # Shuffle and determine split indices\n",
        "    random.shuffle(all_images)\n",
        "    total = len(all_images)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end   = int(total * (train_ratio + val_ratio))\n",
        "\n",
        "    train_imgs = all_images[:train_end]\n",
        "    val_imgs   = all_images[train_end:val_end]\n",
        "    test_imgs  = all_images[val_end:]\n",
        "\n",
        "    print(f\"Total images: {total}\")\n",
        "    print(f\"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n",
        "\n",
        "    # Create output directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_dir, split, 'masks'), exist_ok=True)\n",
        "\n",
        "    # Helper to copy files\n",
        "    def copy_files(file_list, split):\n",
        "        for fname in file_list:\n",
        "            src_img_path = os.path.join(original_dir, fname)\n",
        "            dst_img_path = os.path.join(output_dir, split, 'images', fname)\n",
        "\n",
        "            src_mask_path = os.path.join(mask_dir, fname)\n",
        "            dst_mask_path = os.path.join(output_dir, split, 'masks', fname)\n",
        "\n",
        "            # Copy the original image\n",
        "            shutil.copy2(src_img_path, dst_img_path)\n",
        "            # Copy the corresponding mask\n",
        "            shutil.copy2(src_mask_path, dst_mask_path)\n",
        "\n",
        "    # Copy to train, val, test\n",
        "    copy_files(train_imgs, 'train')\n",
        "    copy_files(val_imgs,   'val')\n",
        "    copy_files(test_imgs,  'test')\n",
        "\n",
        "    print(\"Data splitting complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    original_dir = \"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/Original\"\n",
        "    mask_dir     = \"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/Ground_Truth\"\n",
        "    output_dir   = \"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits\"\n",
        "\n",
        "    create_cvc_clinicdb_splits(\n",
        "        original_dir,\n",
        "        mask_dir,\n",
        "        output_dir,\n",
        "        train_ratio=0.8,\n",
        "        val_ratio=0.1,\n",
        "        test_ratio=0.1,\n",
        "        seed=42\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3D6lwzzDd0H",
        "outputId": "926ad7c7-5b13-4dbe-8c30-d9470a7ef760"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 219\n",
            "Train: 175, Val: 22, Test: 22\n",
            "Data splitting complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import tifffile\n",
        "\n",
        "class PolypDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        image_dir: path to .tif colonoscopy images\n",
        "        mask_dir:  path to matching .tif masks\n",
        "        transform: optional transform that works on tensors or PIL images\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_name)\n",
        "\n",
        "        # Read the .tif image as a NumPy array using tifffile\n",
        "        img_arr = tifffile.imread(img_path)  # shape could be (H,W) or (H,W,C)\n",
        "        mask_arr = tifffile.imread(mask_path)\n",
        "\n",
        "        # Convert them to PyTorch tensors\n",
        "        # If the image is grayscale => shape (H, W). If color => (H, W, 3).\n",
        "        if img_arr.ndim == 2:\n",
        "            # grayscale => expand to (H, W, 1)\n",
        "            img_arr = np.expand_dims(img_arr, axis=-1)\n",
        "        if mask_arr.ndim == 2:\n",
        "            mask_arr = np.expand_dims(mask_arr, axis=-1)\n",
        "\n",
        "        # Convert to float32, scale if needed\n",
        "        img_tensor = torch.from_numpy(img_arr).float()\n",
        "        mask_tensor = torch.from_numpy(mask_arr).float()\n",
        "\n",
        "        # Rearrange from (H, W, C) => (C, H, W)\n",
        "        img_tensor = img_tensor.permute(2, 0, 1)\n",
        "        mask_tensor = mask_tensor.permute(2, 0, 1)\n",
        "\n",
        "        # Threshold the mask\n",
        "        mask_tensor = (mask_tensor > 0.5).float()\n",
        "\n",
        "        return img_tensor, mask_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# U-Net model definition (short version)\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2)\n",
        "        self.conv = DoubleConv(out_channels*2, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # Pad if needed\n",
        "        diffY = x2.size(2) - x1.size(2)\n",
        "        diffX = x2.size(3) - x1.size(3)\n",
        "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX//2, diffY//2, diffY - diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.init_conv = DoubleConv(in_channels, features[0])\n",
        "        self.down1 = DownBlock(features[0], features[1])\n",
        "        self.down2 = DownBlock(features[1], features[2])\n",
        "        self.down3 = DownBlock(features[2], features[3])\n",
        "        self.bottleneck = DoubleConv(features[3], features[3]*2)\n",
        "        self.up1 = UpBlock(features[3]*2, features[3])\n",
        "        self.up2 = UpBlock(features[3], features[2])\n",
        "        self.up3 = UpBlock(features[2], features[1])\n",
        "        self.up4 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n",
        "        self.conv_last = DoubleConv(features[0]*2, features[0])\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.init_conv(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        bottleneck = self.bottleneck(x3)\n",
        "        up1 = self.up1(bottleneck, x3)\n",
        "        up2 = self.up2(up1, x2)\n",
        "        up3 = self.up3(up2, x1)\n",
        "        up4 = self.up4(up3)\n",
        "        # pad if needed\n",
        "        diffY = x0.size(2) - up4.size(2)\n",
        "        diffX = x0.size(3) - up4.size(3)\n",
        "        up4 = nn.functional.pad(up4, [diffX//2, diffX - diffX//2, diffY//2, diffY - diffY//2])\n",
        "        x_out = torch.cat([x0, up4], dim=1)\n",
        "        x_out = self.conv_last(x_out)\n",
        "        return self.final_conv(x_out)\n",
        "\n",
        "# Metrics\n",
        "def compute_segmentation_metrics(pred_mask, true_mask):\n",
        "    pred = pred_mask.view(pred_mask.size(0), -1).float()\n",
        "    truth = true_mask.view(true_mask.size(0), -1).float()\n",
        "    eps = 1e-7\n",
        "    tp = (pred * truth).sum(dim=1)\n",
        "    fp = (pred * (1 - truth)).sum(dim=1)\n",
        "    fn = ((1 - pred) * truth).sum(dim=1)\n",
        "\n",
        "    precision = (tp + eps) / (tp + fp + eps)\n",
        "    recall = (tp + eps) / (tp + fn + eps)\n",
        "    iou = (tp + eps) / (tp + fp + fn + eps)\n",
        "    dice = (2*tp + eps) / (2*tp + fp + fn + eps)\n",
        "\n",
        "    return {\n",
        "        'dice': dice.mean().item(),\n",
        "        'iou': iou.mean().item(),\n",
        "        'precision': precision.mean().item(),\n",
        "        'recall': recall.mean().item()\n",
        "    }\n",
        "\n",
        "# Evaluate function\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_dice, all_iou, all_prec, all_rec = 0, 0, 0, 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            logits = model(images)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "            metrics = compute_segmentation_metrics(preds, masks)\n",
        "            all_dice += metrics['dice']\n",
        "            all_iou  += metrics['iou']\n",
        "            all_prec += metrics['precision']\n",
        "            all_rec  += metrics['recall']\n",
        "            count += 1\n",
        "    return {\n",
        "        'dice': all_dice/count,\n",
        "        'iou': all_iou/count,\n",
        "        'precision': all_prec/count,\n",
        "        'recall': all_rec/count\n",
        "    }\n",
        "\n",
        "# Training loop\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, masks in loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def train_unet(model, train_loader, val_loader, device, epochs=20):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_metrics = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss={train_loss:.4f}, \"\n",
        "              f\"Dice={val_metrics['dice']:.4f}, IoU={val_metrics['iou']:.4f}, \"\n",
        "              f\"Prec={val_metrics['precision']:.4f}, Rec={val_metrics['recall']:.4f}\")\n",
        "\n",
        "# Main\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "    # Prepare data (train, val, test) after splitting\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = PolypDataset(\n",
        "        image_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/train/images\",\n",
        "        mask_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/train/masks\",\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = PolypDataset(\n",
        "        image_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/val/images\",\n",
        "        mask_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/val/masks\",\n",
        "        transform=val_transform\n",
        "    )\n",
        "    test_dataset = PolypDataset(\n",
        "        image_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/test/images\",\n",
        "        mask_dir=\"/content/drive/MyDrive/CSCI_6967/HW5/CVC-ClinicDB/splits/test/masks\",\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=2)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Train\n",
        "    train_unet(model, train_loader, val_loader, device, epochs=20)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_metrics = evaluate(model, test_loader, device)\n",
        "    print(\"Final Test Metrics:\")\n",
        "    print(f\"Dice={test_metrics['dice']:.4f}, IoU={test_metrics['iou']:.4f}, \"\n",
        "          f\"Precision={test_metrics['precision']:.4f}, Recall={test_metrics['recall']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KNo9JctGN58",
        "outputId": "3e060778-fc07-45b0-f7d4-f03aa1955905"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss=0.4362, Dice=0.3264, IoU=0.2534, Prec=0.5485, Rec=0.3685\n",
            "Epoch 2/20, Train Loss=0.3544, Dice=0.3389, IoU=0.2625, Prec=0.6367, Rec=0.3449\n",
            "Epoch 3/20, Train Loss=0.3215, Dice=0.4335, IoU=0.3400, Prec=0.5487, Rec=0.4429\n",
            "Epoch 4/20, Train Loss=0.2940, Dice=0.3682, IoU=0.3054, Prec=0.8798, Rec=0.3347\n",
            "Epoch 5/20, Train Loss=0.2870, Dice=0.4250, IoU=0.3457, Prec=0.6656, Rec=0.3912\n",
            "Epoch 6/20, Train Loss=0.2743, Dice=0.4109, IoU=0.3145, Prec=0.6415, Rec=0.4492\n",
            "Epoch 7/20, Train Loss=0.2525, Dice=0.4268, IoU=0.3287, Prec=0.6907, Rec=0.4707\n",
            "Epoch 8/20, Train Loss=0.2418, Dice=0.4893, IoU=0.4039, Prec=0.7630, Rec=0.4361\n",
            "Epoch 9/20, Train Loss=0.2352, Dice=0.4591, IoU=0.3833, Prec=0.8427, Rec=0.4148\n",
            "Epoch 10/20, Train Loss=0.2136, Dice=0.5200, IoU=0.4275, Prec=0.6227, Rec=0.5211\n",
            "Epoch 11/20, Train Loss=0.2056, Dice=0.5070, IoU=0.4020, Prec=0.7779, Rec=0.4960\n",
            "Epoch 12/20, Train Loss=0.2028, Dice=0.4996, IoU=0.4080, Prec=0.8689, Rec=0.4335\n",
            "Epoch 13/20, Train Loss=0.1844, Dice=0.4804, IoU=0.3945, Prec=0.8465, Rec=0.4330\n",
            "Epoch 14/20, Train Loss=0.1747, Dice=0.5742, IoU=0.4739, Prec=0.6589, Rec=0.5993\n",
            "Epoch 15/20, Train Loss=0.1693, Dice=0.5835, IoU=0.4659, Prec=0.6994, Rec=0.6434\n",
            "Epoch 16/20, Train Loss=0.1630, Dice=0.5504, IoU=0.4387, Prec=0.8078, Rec=0.5602\n",
            "Epoch 17/20, Train Loss=0.1564, Dice=0.5905, IoU=0.5078, Prec=0.7696, Rec=0.5875\n",
            "Epoch 18/20, Train Loss=0.1437, Dice=0.5856, IoU=0.5068, Prec=0.9232, Rec=0.5256\n",
            "Epoch 19/20, Train Loss=0.1533, Dice=0.4818, IoU=0.4195, Prec=0.8326, Rec=0.4678\n",
            "Epoch 20/20, Train Loss=0.1312, Dice=0.6533, IoU=0.5703, Prec=0.8875, Rec=0.6542\n",
            "Final Test Metrics:\n",
            "Dice=0.7111, IoU=0.6104, Precision=0.9120, Recall=0.6848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final results show a Dice score of approximately 0.71 and an IoU of around 0.61, which are notably lower than the 0.88–0.90 (Dice) and 0.82–0.84 (IoU) typically reported by state-of-the-art methods such as PraNet or ResUNet++. This performance gap can arise from factors like limited data augmentation, fewer training epochs, or the absence of additional polyp datasets (e.g., Kvasir-SEG, CVC-ColonDB) that are often used in combination with CVC-ClinicDB. It may also reflect architectural differences, since many high-performing models incorporate advanced features like attention mechanisms or residual connections. Interestingly, my precision is roughly 0.91—on par with or higher than the ~0.88–0.91 cited in the literature, suggesting that the model is skilled at avoiding false positives. However, the recall of about 0.68 indicates it fails to capture some polyp regions, thus reducing its overall Dice and IoU. In essence, the model tends to be conservative (high precision) but misses certain parts of the polyp (lower recall), which explains the discrepancy with more advanced architectures that excel in boundary refinement and small polyp detection.\n",
        "\n",
        "Citations: J. M. J. Valanarasu et al., ResUNet++: An Advanced Architecture for Medical Image Segmentation, D. P. Fan et al., Parallel Reverse Attention Network for Polyp Segmentation, MICCAI 2020."
      ],
      "metadata": {
        "id": "A6AhTvP_baF2"
      }
    }
  ]
}