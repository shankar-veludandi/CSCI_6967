{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "fnigmeNjXkce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uqx8d339UmQN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a2a56936-b54c-41ac-91df-92abe1b804e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport math\\nimport numpy as np\\n\\ndef scaled_dot_product_attention(Q, K, V, mask=None):\\n    \"\"\"\\n    Compute scaled dot-product attention.\\n\\n    :param Q: Query matrix of shape (..., seq_len_q, d_k)\\n    :param K: Key matrix of shape (..., seq_len_k, d_k)\\n    :param V: Value matrix of shape (..., seq_len_k, d_v)\\n    :param mask: (Optional) broadcastable mask tensor to\\n                 apply on the attention logits before softmax\\n    :return: (output, attention_weights)\\n    \"\"\"\\n\\n    # 1. Calculate the dot products between Q and K^T\\n    #    shape of QK^T => (..., seq_len_q, seq_len_k)\\n    d_k = Q.shape[-1]\\n    scores = np.matmul(Q, np.transpose(K, axes=[0, 1, 3, 2])) / math.sqrt(d_k)\\n\\n    # 2. (Optional) Apply the mask: set masked positions to a large negative value\\n    if mask is not None:\\n        scores = scores + (mask * -1e9)  # or float(\\'-inf\\') if supported\\n\\n    # 3. Softmax over the last dimension to get attention weights\\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\\n\\n    # 4. Multiply by V to get the weighted sum\\n    #    shape => (..., seq_len_q, d_v)\\n    output = np.matmul(attention_weights, V)\\n\\n    return output, attention_weights\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "'''\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    :param Q: Query matrix of shape (..., seq_len_q, d_k)\n",
        "    :param K: Key matrix of shape (..., seq_len_k, d_k)\n",
        "    :param V: Value matrix of shape (..., seq_len_k, d_v)\n",
        "    :param mask: (Optional) broadcastable mask tensor to\n",
        "                 apply on the attention logits before softmax\n",
        "    :return: (output, attention_weights)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate the dot products between Q and K^T\n",
        "    #    shape of QK^T => (..., seq_len_q, seq_len_k)\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.matmul(Q, np.transpose(K, axes=[0, 1, 3, 2])) / math.sqrt(d_k)\n",
        "\n",
        "    # 2. (Optional) Apply the mask: set masked positions to a large negative value\n",
        "    if mask is not None:\n",
        "        scores = scores + (mask * -1e9)  # or float('-inf') if supported\n",
        "\n",
        "    # 3. Softmax over the last dimension to get attention weights\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # 4. Multiply by V to get the weighted sum\n",
        "    #    shape => (..., seq_len_q, d_v)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "oekCAvHPXm37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        # batch_first=True => input shape (batch, seq_len)\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        :param src: (batch_size, src_len) of token indices\n",
        "        :return:\n",
        "          - outputs: (batch_size, src_len, hidden_dim)\n",
        "          - (h, c): final hidden/cell states for each layer\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(src)  # (batch, src_len, embed_dim)\n",
        "        outputs, (h, c) = self.lstm(embedded)  # outputs: (batch, src_len, hidden_dim)\n",
        "        return outputs, (h, c)\n"
      ],
      "metadata": {
        "id": "vul3soWAXoDH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)  # combine hidden + context\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        :param input_token: (batch_size,) current input word index\n",
        "        :param hidden, cell: decoder's LSTM hidden/cell states\n",
        "        :param encoder_outputs: (batch_size, src_len, hidden_dim)\n",
        "        :return:\n",
        "          - output logits: (batch_size, output_dim)\n",
        "          - hidden, cell: updated states\n",
        "          - attn_weights: (batch_size, 1, src_len)\n",
        "        \"\"\"\n",
        "        # 1) Embed input token\n",
        "        input_token = input_token.unsqueeze(1)   # (batch_size, 1)\n",
        "        embedded = self.embedding(input_token)   # (batch_size, 1, embed_dim)\n",
        "\n",
        "        # 2) Pass embedded token + previous hidden state into LSTM\n",
        "        #    hidden, cell each is (num_layers, batch_size, hidden_dim)\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # lstm_output shape: (batch_size, 1, hidden_dim)\n",
        "\n",
        "        # 3) Compute scaled dot-product attention\n",
        "        #    Q = lstm_output, K=encoder_outputs, V=encoder_outputs\n",
        "        #    But we need them in shape (batch, seq_len, d_model).\n",
        "        #    Here, d_model = hidden_dim for simplicity\n",
        "        Q = lstm_output  # (batch, 1, hidden_dim)\n",
        "        K = encoder_outputs  # (batch, src_len, hidden_dim)\n",
        "        V = encoder_outputs  # (batch, src_len, hidden_dim)\n",
        "\n",
        "        # scaled_dot_product_attention expects (batch, seq_q, d_model)\n",
        "        # and (batch, seq_k, d_model). That is correct for Q, K, V.\n",
        "        context, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
        "        # context: (batch, 1, hidden_dim)\n",
        "        # attn_weights: (batch, 1, src_len)\n",
        "\n",
        "        # 4) Concatenate context and LSTM output, then predict next token\n",
        "        #    shape => (batch, 1, hidden_dim*2)\n",
        "        combined = torch.cat((lstm_output, context), dim=-1)\n",
        "        # produce a distribution over output vocab\n",
        "        logits = self.fc_out(combined.squeeze(1))  # (batch_size, output_dim)\n",
        "\n",
        "        return logits, hidden, cell, attn_weights\n"
      ],
      "metadata": {
        "id": "YKbQ4Ea7a63H"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        src: (batch_size, src_len)\n",
        "        trg: (batch_size, trg_len) -- for training\n",
        "        returns: logits for each trg token\n",
        "                 shape => (batch_size, trg_len, output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.size()\n",
        "        # Encode source\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
        "\n",
        "        # Prepare a place to store decoder predictions\n",
        "        output_dim = self.decoder.fc_out.out_features\n",
        "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n",
        "\n",
        "        # First input token to decoder is typically the <sos> (start) token\n",
        "        input_token = trg[:, 0]  # (batch_size,)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Pass the input token + hidden states + encoder outputs\n",
        "            logits, hidden, cell, attn_weights = self.decoder(input_token, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Pick next token (for *teacher forcing*, we often feed the true token)\n",
        "            # or do greedy decoding. For training with teacher forcing:\n",
        "            input_token = trg[:, t]\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "NLFNobnTa9vt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "uqS7Aba7biNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIov8HbbfXO3",
        "outputId": "0bd82e45-92f9-4cf8-945f-782e6db16ddc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For demonstration, reusing placeholders from earlier code\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "EQHlHL3fbjTa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Helsinki-NLP/tatoeba\", \"en-mr\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYGM_N5ydzgE",
        "outputId": "670f539c-c338-45b8-ba93-b7ac94c22e0a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'translation'],\n",
            "        num_rows: 53462\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = dataset[\"train\"]  # All Tatoeba sentence pairs for en-mr\n",
        "\n",
        "# Convert to a Python list of dicts\n",
        "raw_list = list(raw_data)\n",
        "\n",
        "# subset for demonstration\n",
        "raw_list = raw_list[:5000]\n",
        "\n",
        "# Now split into train/val/test using scikit-learn\n",
        "train_data, test_data = train_test_split(raw_list, test_size=0.1, random_state=42)\n",
        "train_data, val_data  = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Train size:\", len(train_data))\n",
        "print(\"Val size:  \", len(val_data))\n",
        "print(\"Test size: \", len(test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeT-krYwd6eJ",
        "outputId": "dbdd169f-07db-4e8c-8729-22ad8ef0e27d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 4050\n",
            "Val size:   450\n",
            "Test size:  500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    # Very naive approach\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"[^\\p{L}\\p{N}'-]+\", \" \", sentence, flags=re.UNICODE)\n",
        "    tokens = sentence.strip().split()\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(tokenized_sents, min_freq=2):\n",
        "    from collections import Counter\n",
        "    freq = Counter()\n",
        "    for sent in tokenized_sents:\n",
        "        for w in sent:\n",
        "            freq[w] += 1\n",
        "    # Basic special tokens\n",
        "    words = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "    for w, c in freq.items():\n",
        "        if c >= min_freq:\n",
        "            words.append(w)\n",
        "    word2idx = {w: i for i, w in enumerate(words)}\n",
        "    return word2idx\n",
        "\n",
        "def numericalize(tokens, word2idx):\n",
        "    return [word2idx.get(t, word2idx[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "def process_pairs(data_list):\n",
        "    en_tok, mr_tok = [], []\n",
        "    for d in data_list:\n",
        "        # 'translation' is a list: [english_text, marathi_text]\n",
        "        en_sent = tokenize(d[\"translation\"]['en'])  # English\n",
        "        mr_sent = tokenize(d[\"translation\"]['mr'])  # Marathi\n",
        "        en_tok.append(en_sent)\n",
        "        mr_tok.append(mr_sent)\n",
        "    return en_tok, mr_tok\n",
        "\n"
      ],
      "metadata": {
        "id": "coWFYOxud89b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_tok_train, mr_tok_train = process_pairs(train_data)\n",
        "en_tok_val,   mr_tok_val   = process_pairs(val_data)\n",
        "en_tok_test,  mr_tok_test  = process_pairs(test_data)\n",
        "\n",
        "en_word2idx = build_vocab(en_tok_train, min_freq=2)\n",
        "mr_word2idx = build_vocab(mr_tok_train, min_freq=2)\n",
        "\n",
        "def add_sos_eos(token_ids, sos_idx, eos_idx):\n",
        "    return [sos_idx] + token_ids + [eos_idx]\n",
        "\n",
        "def to_idx_pairs(en_tok, mr_tok):\n",
        "    pairs = []\n",
        "    for e, m in zip(en_tok, mr_tok):\n",
        "        e_ids = numericalize(e, en_word2idx)\n",
        "        m_ids = numericalize(m, mr_word2idx)\n",
        "        # add <sos>, <eos> (we assume <sos>=1, <eos>=2 in that order)\n",
        "        e_ids = add_sos_eos(e_ids, 1, 2)\n",
        "        m_ids = add_sos_eos(m_ids, 1, 2)\n",
        "        pairs.append((e_ids, m_ids))\n",
        "    return pairs\n",
        "\n",
        "train_pairs = to_idx_pairs(en_tok_train, mr_tok_train)\n",
        "val_pairs   = to_idx_pairs(en_tok_val,   mr_tok_val)\n",
        "test_pairs  = to_idx_pairs(en_tok_test,  mr_tok_test)\n",
        "\n",
        "print(f\"train_pairs: {len(train_pairs)}, val_pairs: {len(val_pairs)}, test_pairs: {len(test_pairs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MVI2bCdeAAG",
        "outputId": "b871971b-282b-4189-9738-ecb954dd93b5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_pairs: 4050, val_pairs: 450, test_pairs: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MTDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_list, trg_list = zip(*batch)\n",
        "    max_src_len = max(len(s) for s in src_list)\n",
        "    max_trg_len = max(len(t) for t in trg_list)\n",
        "    padded_src, padded_trg = [], []\n",
        "    for s, t in zip(src_list, trg_list):\n",
        "        s_pad = s + [0]*(max_src_len - len(s))  # 0 = <pad>\n",
        "        t_pad = t + [0]*(max_trg_len - len(t))\n",
        "        padded_src.append(s_pad)\n",
        "        padded_trg.append(t_pad)\n",
        "    return torch.tensor(padded_src, dtype=torch.long), torch.tensor(padded_trg, dtype=torch.long)\n",
        "\n",
        "train_dataset = MTDataset(train_pairs)\n",
        "val_dataset   = MTDataset(val_pairs)\n",
        "test_dataset  = test_pairs  # we can keep test as list for direct BLEU\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "PhtgERlxeOsd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    return output, attn_weights\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for src_batch, trg_batch in train_loader:\n",
        "            src_batch, trg_batch = src_batch.to(model.device), trg_batch.to(model.device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(src_batch, trg_batch)\n",
        "            logits = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
        "            targets = trg_batch[:, 1:].reshape(-1)\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_loss = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch}, Train Loss: {avg_loss:.3f}, Val Loss: {val_loss:.3f}\")\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in loader:\n",
        "            src_batch, trg_batch = src_batch.to(model.device), trg_batch.to(model.device)\n",
        "            outputs = model(src_batch, trg_batch)\n",
        "            logits = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
        "            targets = trg_batch[:, 1:].reshape(-1)\n",
        "            loss = criterion(logits, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "Jj5e-Wl2eQ5s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "en_vocab_size = len(en_word2idx)\n",
        "mr_vocab_size = len(mr_word2idx)\n",
        "\n",
        "encoder = Encoder(en_vocab_size, embed_dim=128, hidden_dim=256)\n",
        "decoder = Decoder(mr_vocab_size, embed_dim=128, hidden_dim=256)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dzlah7GeRq6",
        "outputId": "d1feccae-61d6-4df1-c290-175462ce8a07"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 4.127, Val Loss: 3.686\n",
            "Epoch 2, Train Loss: 3.529, Val Loss: 3.342\n",
            "Epoch 3, Train Loss: 3.178, Val Loss: 3.130\n",
            "Epoch 4, Train Loss: 2.878, Val Loss: 2.929\n",
            "Epoch 5, Train Loss: 2.597, Val Loss: 2.774\n",
            "Epoch 6, Train Loss: 2.330, Val Loss: 2.642\n",
            "Epoch 7, Train Loss: 2.088, Val Loss: 2.533\n",
            "Epoch 8, Train Loss: 1.861, Val Loss: 2.459\n",
            "Epoch 9, Train Loss: 1.657, Val Loss: 2.411\n",
            "Epoch 10, Train Loss: 1.468, Val Loss: 2.358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def translate_sentence(model, src_ids, max_len=50):\n",
        "    model.eval()\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=model.device).unsqueeze(0)\n",
        "    encoder_outputs, (h, c) = model.encoder(src_tensor)\n",
        "    input_token = torch.tensor([1], device=model.device)  # <sos>=1\n",
        "    hidden, cell = h, c\n",
        "    preds = [1]\n",
        "    for _ in range(max_len):\n",
        "        logits, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
        "        next_token = logits.argmax(dim=-1)\n",
        "        next_id = next_token.item()\n",
        "        preds.append(next_id)\n",
        "        if next_id == 2:  # <eos>=2\n",
        "            break\n",
        "        input_token = next_token\n",
        "    return preds\n",
        "\n",
        "def compute_bleu(model, test_data, fr_idx2word):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (src_ids, trg_ids) in test_data:\n",
        "            # remove <sos>=1, <eos>=2 from reference\n",
        "            gold = [w for w in trg_ids[1:-1] if w not in [0,1,2]]\n",
        "            pred = translate_sentence(model, src_ids)\n",
        "            # remove <sos>, <eos> from pred\n",
        "            pred = [w for w in pred if w not in [0,1,2]]\n",
        "            references.append([gold])\n",
        "            hypotheses.append(pred)\n",
        "    bleu = corpus_bleu(references, hypotheses) * 100\n",
        "    return bleu\n",
        "\n",
        "# Index->word\n",
        "mr_idx2word = {v: k for k, v in mr_word2idx.items()}\n",
        "\n",
        "bleu_score = compute_bleu(model, test_dataset, mr_idx2word)\n",
        "print(f\"Test BLEU: {bleu_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hdjnwYAe3vp",
        "outputId": "ec2e6022-4fd7-43b8-8211-d81227570b70"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test BLEU: 15.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4"
      ],
      "metadata": {
        "id": "Lso8ZinU7KgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape => (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qVHnphd47L_-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # final projection\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q, K, V: (batch, seq_len, d_model)\n",
        "        mask: broadcastable bool mask (True=ignore)\n",
        "        \"\"\"\n",
        "        B, Lq, D = Q.shape\n",
        "        _, Lk, _ = K.shape\n",
        "\n",
        "        # 1) Project Q, K, V\n",
        "        q = self.W_q(Q)\n",
        "        k = self.W_k(K)\n",
        "        v = self.W_v(V)\n",
        "\n",
        "        # 2) Reshape for multi-head\n",
        "        q = q.view(B, Lq, self.num_heads, self.d_k).transpose(1, 2)  # (B, heads, L, d_k)\n",
        "        k = k.view(B, Lk, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(B, Lk, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "\n",
        "        # Flatten batch for scaled dot-product attention\n",
        "        Bnh = B * self.num_heads\n",
        "        q = q.contiguous().view(Bnh, Lq, self.d_k)\n",
        "        k = k.contiguous().view(Bnh, Lk, self.d_k)\n",
        "        v = v.contiguous().view(Bnh, Lk, self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Expand mask for multiple heads if needed\n",
        "            mask = mask.repeat(self.num_heads, 1, 1, 1)\n",
        "\n",
        "        # 3) Call your scaled_dot_product_attention\n",
        "        output, attn_weights = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "\n",
        "\n",
        "        # 4) Reshape back\n",
        "        output = output.view(B, self.num_heads, Lq, self.d_k).transpose(1, 2).contiguous()\n",
        "        output = output.view(B, Lq, D)\n",
        "\n",
        "        # 5) Final projection\n",
        "        output = self.W_o(output)\n",
        "        return output, attn_weights\n"
      ],
      "metadata": {
        "id": "cgAFIKHd8PJX"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model=64, dim_feedforward=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.fc2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x => (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n"
      ],
      "metadata": {
        "id": "ONCTdenT9OmZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, dim_feedforward=128):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, dim_feedforward)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.mha(x, x, x, mask=src_mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        # Feed-forward\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "k0MCh9TV8Qmt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, dim_feedforward=128):\n",
        "        super().__init__()\n",
        "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, dim_feedforward)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, src_mask=None):\n",
        "        # 1) Masked self-attention\n",
        "        out1, _ = self.self_mha(x, x, x, mask=tgt_mask)\n",
        "        x = self.norm1(x + out1)\n",
        "        # 2) Cross-attention\n",
        "        out2, attn_weights = self.cross_mha(x, enc_out, enc_out, mask=src_mask)\n",
        "        x = self.norm2(x + out2)\n",
        "        # 3) Feed-forward\n",
        "        out3 = self.ffn(x)\n",
        "        x = self.norm3(x + out3)\n",
        "        return x, attn_weights\n"
      ],
      "metadata": {
        "id": "QfzDZlop8Srz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, num_heads=2, dim_feedforward=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, dim_feedforward)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # src => (batch, src_len)\n",
        "        x = self.embedding(src)  # (batch, src_len, d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, num_heads=2, dim_feedforward=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, dim_feedforward)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, src_mask=None):\n",
        "        # tgt => (batch, tgt_len)\n",
        "        x = self.embedding(tgt)\n",
        "        x = self.pos_encoding(x)\n",
        "        attn_weights = None\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights = layer(x, enc_out, tgt_mask, src_mask)\n",
        "        logits = self.fc_out(x)  # (batch, tgt_len, vocab_size)\n",
        "        return logits, attn_weights\n"
      ],
      "metadata": {
        "id": "ib4ufkVY-AGl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        d_model=64,\n",
        "        num_heads=2,\n",
        "        dim_feedforward=128,\n",
        "        num_layers=2,\n",
        "        device=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, dim_feedforward, num_layers)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, dim_feedforward, num_layers)\n",
        "\n",
        "    def make_subsequent_mask(self, sz):\n",
        "        \"\"\"\n",
        "        Creates a mask for the subsequent tokens (True=masked).\n",
        "        shape => (1, sz, sz)\n",
        "        \"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # 1) Encode\n",
        "        enc_out = self.encoder(src)\n",
        "        # 2) Create target mask for autoregressive decoding\n",
        "        B, tgt_len = tgt.size()\n",
        "        tgt_mask = self.make_subsequent_mask(tgt_len).to(tgt.device)\n",
        "        # 3) Decode\n",
        "        logits, attn_weights = self.decoder(tgt, enc_out, tgt_mask, src_mask=None)\n",
        "        return logits, attn_weights\n"
      ],
      "metadata": {
        "id": "KhzT9KnT-BgF"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_tok_train, mr_tok_train = process_pairs(train_data)\n",
        "en_tok_val,   mr_tok_val   = process_pairs(val_data)\n",
        "en_tok_test,  mr_tok_test  = process_pairs(test_data)\n",
        "\n",
        "en_word2idx = build_vocab(en_tok_train, min_freq=2)\n",
        "mr_word2idx = build_vocab(mr_tok_train, min_freq=2)\n",
        "\n",
        "train_pairs = to_idx_pairs(en_tok_train, mr_tok_train)\n",
        "val_pairs   = to_idx_pairs(en_tok_val,   mr_tok_val)\n",
        "test_pairs  = to_idx_pairs(en_tok_test,  mr_tok_test)\n",
        "\n",
        "train_dataset = MTDataset(train_pairs)\n",
        "val_dataset   = MTDataset(val_pairs)\n",
        "test_dataset  = test_pairs  # we can keep test as list for direct BLEU\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "en_vocab_size = len(en_word2idx)\n",
        "mr_vocab_size = len(mr_word2idx)\n",
        "\n",
        "model = TransformerModel(\n",
        "    src_vocab_size=en_vocab_size,\n",
        "    tgt_vocab_size=mr_vocab_size,\n",
        "    d_model=64,\n",
        "    num_heads=2,\n",
        "    dim_feedforward=128,\n",
        "    num_layers=2,\n",
        "    device=device\n",
        ").to(device)\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=10)\n",
        "\n",
        "mr_idx2word = {v: k for k, v in mr_word2idx.items()}\n",
        "\n",
        "bleu_score = compute_bleu(model, test_dataset, mr_idx2word)\n",
        "print(f\"Test BLEU: {bleu_score:.2f}\")"
      ],
      "metadata": {
        "id": "Rttpo2cG-Hw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "5948513b-d77d-42d0-df93-91c3bfb4ffe4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[32, 2, 28, 32]' is invalid for input of size 114688",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-990317b335a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m ).to(device)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmr_idx2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmr_word2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-adfed264d75a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0msrc_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-54fbe6cf3de0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# 3) Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-ee384246d069>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, enc_out, tgt_mask, src_mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, tgt_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-52890fb543f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_out, tgt_mask, src_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 1) Masked self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_mha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# 2) Cross-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-1e4e398ba8dd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# 4) Reshape back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 2, 28, 32]' is invalid for input of size 114688"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could not figure out how to debug this error. There is a shape mismatch and I already ensured that the mask I pass for cross-attention is not already repeated and that my multi-head attention logic handles L(Q) and L(K,V) separately. Besides that, I believe my code meets all the requirements outlined in Task 3 part 4."
      ],
      "metadata": {
        "id": "PbC9FWaGjq5Z"
      }
    }
  ]
}